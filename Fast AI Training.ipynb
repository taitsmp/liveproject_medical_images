{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fastai 3D images\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "* https://towardsdatascience.com/working-with-3d-data-fastai2-5e2baa09037e \n",
    "* https://www.kaggle.com/jhoward/some-dicom-gotchas-to-be-aware-of-fastai\n",
    "* https://forums.fast.ai/t/fastai-v2-has-a-medical-imaging-submodule/56117\n",
    "* https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5\n",
    "\n",
    "Fastai\n",
    "\n",
    "* Pipelines and Transforms - https://docs.fast.ai/tutorial.pets\n",
    "* Info on batch and item transforms - https://docs.fast.ai/vision.data#ImageDataLoaders\n",
    "  - item transforms happen one item at a time on the CPU\n",
    "  - batch transforms happen one batch at a time on the GPU\n",
    "* Blocks tutorial - https://docs.fast.ai/tutorial.datablock.html\n",
    "\n",
    "\n",
    "Pytorch\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "Note that Fastai defaults to DICOM and doesn't appear to support NIFTI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Most of your types Blocks extend or compose TransformBlock.  They just call type, item and batch transforms.\n",
    "* Pipelines would call a bunch of transforms at one time. \n",
    "* Transforms seem to be the key thing. consider writing these correctly first.\n",
    "* Once you have the transforms you can figure out if you want to use a TfmsList or Dataset or DataBlock. \n",
    "* Consider inheriting from \"ItemTransform\". Can you see if you can get your to batch to \"show\" by writing \"decode\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a DataBlock. \n",
    "\n",
    "It will give you back a Dataset and DataLoader\n",
    "\n",
    "\n",
    "To build a DataBlock you need to give the library four things: \n",
    " \n",
    "* the types of your input/labels, and at least two functions: `get_items` and `splitter`. \n",
    "* You may also need to include `get_x` and `get_y` or a more generic list of getters that are applied to the results of get_items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageSequenceBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-90819e3b2e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dblock = DataBlock(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mblocks\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImageSequenceBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoryBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mget_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceGetItems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sequence_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mget_x\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageSequenceBlock' is not defined"
     ]
    }
   ],
   "source": [
    "#example DataBlock - won't run.\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks    = (ImageSequenceBlock, CategoryBlock),\n",
    "    get_items = SequenceGetItems('file', 'sequence_id', 'label'), \n",
    "    get_x     = lambda t : t[:-1],\n",
    "    get_y     = lambda t : t[-1],\n",
    "    splitter  = RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the definition of an ImageBlock in the fast.ai codebase.\n",
    "def ImageBlock(cls=PILImage):\n",
    "    \"A `TransformBlock` for images of `cls`\"\n",
    "    return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the format of get_items? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.fast.ai/data.transforms.html#get_files\n",
    "files = get_files('./data/small/t1', extensions='.gz', recurse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('data/small/t1/IXI167-HH-1569-T1_fcm.nii.gz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastcore.foundation.L"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first link in the resource section:\n",
    "\n",
    "> remember from the DataBlock example above that get_x and get_y receive the output from get_items and should separate what is the input and what is the target. In this case, they are as simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NiftiGetItems():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, source_dir):\n",
    "        \n",
    "        #TODO: could just take the parent directory\n",
    "        t1_files = os.listdir(source_dir)\n",
    "\n",
    "        subj_list = [ path_to_subj(e) for e in t1_files if fnmatch(e, r'*T1_fcm.nii.gz') ] \n",
    "        \n",
    "        out = [] \n",
    "        \n",
    "        for s in subj_list:\n",
    "            t1 = load_mr_image(s, \"t1\")\n",
    "            t2 = load_mr_image(s, \"t2\")           \n",
    "            out.append({'source': t1, 'target': t2})\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#https://fastcore.fast.ai/transform#Transform (how do these differen from pytorch transforms?)\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
    "class ToTensor(ItemTransform):\n",
    "    \n",
    "    def setups(self, items):\n",
    "        pass\n",
    "    \n",
    "    def encodes(self, o):\n",
    "        pass\n",
    "    \n",
    "    def decodes(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "def pathsToNifiObjs(image_file_tuples):\n",
    "    #Note: this could be turned into a transform. \n",
    "    #Note: you may be able to skip this. Only works if you refactor the NiftiGetItems() to return a path. \n",
    "    pass\n",
    "    \n",
    "def create_batch(data):\n",
    "    \n",
    "    print('in create_batch')\n",
    "    print(data)\n",
    "    print(len(data))\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for s in data:\n",
    "        print('calling TT')\n",
    "        t = ToTensor(s) #this is wrong. Need to construct an obj then call it. \n",
    "        print('got a tensor')\n",
    "        xs.append(t['source'])\n",
    "        ys.append(t['target'])\n",
    "    print('leaving create_batch')\n",
    "    return (xs, ys)\n",
    "\n",
    "#question does DataLoader do the transform to a tensor in pytorch? Not Dataset?\n",
    "\n",
    "\n",
    "#This class is only needed if you decide to make GetItems just return paths. \n",
    "def NiftiBlock():\n",
    "    return TransformBlock(item_tfms=NiftiToTensor(),\n",
    "                          #batch_tfms= crop or data augment here...\n",
    "                          #type_tfms= pathsToNiftiObjs\n",
    "                         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    #blocks    = NiftiBlock(),\n",
    "    #blocks    = [TransformBlock(item_tfms=[ToNumpy(), AddDim(),])], #this is the wrong format. should be (sourceBlock, targetblock). This could maybe be a pipeline. \n",
    "    get_items = NiftiGetItems(),\n",
    "    get_x     = lambda t : t['source'], \n",
    "    get_y     = lambda t : t['target'],\n",
    "    #n_inp = how many inputs do you want to pass into the datablock / get_items, etc. \n",
    "    splitter  = RandomSplitter())\n",
    "\n",
    "#TODO: See above. \n",
    "# - crop is not happening. Fix this. \n",
    "# - not creating tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = dblock.datasets('./data/small/t1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#28) [(<nibabel.nifti1.Nifti1Image object at 0x7f9dcce2aa30>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf2069a0>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf2063a0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf206c10>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf4837f0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf25c5e0>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf4836a0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf211370>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf2112b0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf211040>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf211dc0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf483820>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf211280>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf211d60>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf1ab1f0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf1ab070>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf1ab280>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf1ab4f0>),(<nibabel.nifti1.Nifti1Image object at 0x7f9caf1abee0>, <nibabel.nifti1.Nifti1Image object at 0x7f9caf1b0220>)...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in create_batch\n",
      "[(<nibabel.nifti1.Nifti1Image object at 0x7efaeea5f400>, <nibabel.nifti1.Nifti1Image object at 0x7efaeea59190>)]\n",
      "1\n",
      "calling TT\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n"
     ]
    }
   ],
   "source": [
    "dls = dblock.dataloaders('./data/small/t1', bs=8, create_batch=create_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data.core.Datasets"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<nibabel.nifti1.Nifti1Image at 0x7ffa7d0e12e0>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x7ffa7d0e1550>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ToTensor\n",
      "here\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Nifti1Image' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4b0168a3ab17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/med-image-analysis-manning-lp/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mnp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mnp_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Nifti1Image' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "ToTensor()(dsets.train[0])\n",
    "\n",
    "#LEFT OFF HERE. NOT SURE WHY THIS DIDN'T WORK. TRY A TRANSFORMED LIST (TfmdList). See the Pipeline and transforms tutorial. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ToTensor\n",
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.1064e-28,  2.6875e-23,  1.5517e-19,  ..., -6.5872e-18,\n",
       "             4.7015e-18, -1.4260e-21],\n",
       "           [ 1.0044e-18,  6.2352e-18,  2.8831e-17,  ...,  1.7370e-10,\n",
       "            -2.5201e-11,  2.3136e-14],\n",
       "           [ 2.8891e-17,  1.7935e-16,  5.1307e-15,  ...,  4.2897e-09,\n",
       "             4.9453e-11,  6.6545e-13],\n",
       "           ...,\n",
       "           [ 4.1844e-19, -1.3712e-07, -3.9062e-06,  ...,  4.7797e-05,\n",
       "            -3.0614e-06, -1.3534e-08],\n",
       "           [ 5.2593e-18, -3.5583e-06, -1.0138e-04,  ...,  1.2287e-03,\n",
       "            -7.8700e-05, -3.4792e-07],\n",
       "           [-2.3027e-18, -1.9536e-07, -5.6225e-06,  ..., -4.1198e-06,\n",
       "             2.6404e-07,  1.1666e-09]],\n",
       " \n",
       "          [[-2.3321e-15, -9.7401e-15, -2.7863e-13,  ..., -4.2805e-13,\n",
       "             7.5283e-12,  2.2763e-16],\n",
       "           [ 6.0129e-16,  1.5016e-11,  4.3153e-10,  ...,  1.3986e-04,\n",
       "            -1.6346e-05, -7.2220e-08],\n",
       "           [-1.8859e-15,  1.7291e-12,  4.7141e-11,  ...,  5.8584e-04,\n",
       "            -3.6260e-05, -1.6031e-07],\n",
       "           ...,\n",
       "           [ 3.0096e-19,  8.9810e-10,  7.9199e-08,  ...,  1.5666e-05,\n",
       "            -1.0032e-06, -4.4354e-09],\n",
       "           [ 1.2735e-18,  5.4928e-08,  1.4894e-06,  ...,  3.8764e-04,\n",
       "            -2.4828e-05, -1.0976e-07],\n",
       "           [-3.4527e-17, -2.3411e-06, -6.6507e-05,  ...,  3.6032e-04,\n",
       "            -2.3078e-05, -1.0203e-07]],\n",
       " \n",
       "          [[ 2.4792e-14,  1.9903e-13,  5.7088e-12,  ...,  6.7425e-14,\n",
       "            -1.3980e-16, -1.4282e-15],\n",
       "           [-7.0600e-15,  1.4173e-10,  4.0730e-09,  ..., -2.3679e-05,\n",
       "             8.9087e-07,  3.9420e-09],\n",
       "           [ 1.8045e-14,  1.6625e-11,  4.5362e-10,  ...,  9.0370e-05,\n",
       "            -5.6510e-06, -2.4984e-08],\n",
       "           ...,\n",
       "           [-7.4745e-20, -3.0748e-08, -6.9472e-07,  ...,  4.7425e-06,\n",
       "            -3.1122e-07, -1.3769e-09],\n",
       "           [-1.1296e-18,  1.6416e-07,  4.4350e-06,  ...,  6.6745e-05,\n",
       "            -4.2752e-06, -1.8901e-08],\n",
       "           [-2.3169e-17,  3.6669e-07,  1.0407e-05,  ...,  1.0880e-03,\n",
       "            -6.9685e-05, -3.0808e-07]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.8728e-15,  1.1958e-08,  3.4069e-07,  ...,  1.5845e-10,\n",
       "            -2.1056e-10,  3.1186e-14],\n",
       "           [ 9.0112e-15, -4.7256e-08, -1.3829e-06,  ...,  4.5777e-04,\n",
       "             2.4139e-05,  1.0645e-07],\n",
       "           [ 2.5947e-14, -1.8863e-07, -6.5037e-06,  ...,  7.4095e-04,\n",
       "            -8.0737e-05, -3.5675e-07],\n",
       "           ...,\n",
       "           [-9.4080e-14,  9.8668e-06,  3.5394e-04,  ..., -1.3912e-06,\n",
       "            -4.7510e-08, -2.0936e-10],\n",
       "           [-2.6296e-14,  2.9358e-07,  1.1740e-05,  ..., -3.1351e-06,\n",
       "            -9.7372e-08, -4.2912e-10],\n",
       "           [-9.4943e-14,  3.1765e-10,  2.3642e-08,  ...,  1.3296e-05,\n",
       "             5.8693e-07,  2.5865e-09]],\n",
       " \n",
       "          [[-1.6726e-15,  2.3538e-09,  6.7044e-08,  ...,  5.9616e-11,\n",
       "             1.1989e-10, -1.5426e-15],\n",
       "           [ 6.2562e-15, -9.5099e-09, -2.7800e-07,  ...,  1.4110e-03,\n",
       "            -3.2155e-05, -1.4244e-07],\n",
       "           [ 2.2166e-14, -3.6734e-08, -1.2656e-06,  ...,  1.9627e-03,\n",
       "            -5.3997e-05, -2.3907e-07],\n",
       "           ...,\n",
       "           [ 1.8326e-15,  7.9592e-06,  2.8725e-04,  ..., -2.4990e-05,\n",
       "            -8.7495e-07, -3.8556e-09],\n",
       "           [-3.8305e-14, -4.2417e-07, -1.4350e-05,  ..., -7.3201e-05,\n",
       "            -2.4787e-06, -1.0923e-08],\n",
       "           [-9.6744e-14, -1.9365e-09, -6.5136e-08,  ..., -8.3162e-05,\n",
       "            -3.6611e-06, -1.6134e-08]],\n",
       " \n",
       "          [[-6.9161e-20, -1.0098e-10, -2.8791e-09,  ...,  1.5970e-12,\n",
       "            -5.4854e-14,  6.5935e-16],\n",
       "           [ 8.3910e-20,  2.1276e-10,  5.3049e-09,  ...,  1.1317e-04,\n",
       "            -4.6338e-06, -2.0486e-08],\n",
       "           [-7.6319e-20, -3.0528e-09, -1.0814e-07,  ...,  1.2776e-04,\n",
       "            -8.5598e-06, -3.7818e-08],\n",
       "           ...,\n",
       "           [-1.0698e-14, -2.2073e-07, -7.9966e-06,  ...,  2.0016e-04,\n",
       "             7.0835e-06,  3.1195e-08],\n",
       "           [ 2.0182e-14,  1.3524e-08,  4.5335e-07,  ...,  6.6894e-04,\n",
       "             2.3787e-05,  1.0476e-07],\n",
       "           [-9.5770e-15,  5.1343e-09,  1.4797e-07,  ...,  8.3917e-06,\n",
       "             3.1574e-07,  1.3890e-09]]]]),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dsets.train[0]\n",
    "#TODO: turn this into a TransformBlock ? \n",
    "b = ToNumpy()(a)  \n",
    "c = AddDim()(b)\n",
    "d = ToTensor()(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the transforms I'd need to run on nifti.  \n",
    "#can these be part of a TransformBlock? \n",
    "#would they be batch transforms? \n",
    "#when does the tansform get called in vanilla pytorch? just when you construct a batch? \n",
    "transforms.Compose([RandomCrop3D(cube_crop_dim), \n",
    "                                                            ToNumpy(), \n",
    "                                                            AddDim(), \n",
    "                                                            ToTensor()]),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-v2",
   "language": "python",
   "name": "fastai-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
