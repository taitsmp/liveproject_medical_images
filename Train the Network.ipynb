{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Split the dataset into a training, validation, and test set by randomly moving some percentage of the provided data into a validation and test directoryâ€”split the data evenly between the two, but make the training set the largest. Make sure that the training, validation, and test images do no overlap.\n",
    "1. Use the `DataLoader` class to create the loading mechanism for the training and validation data using the `Dataset` class built in Step 2.\n",
    "1. Build a training loop using MSE as the loss function. Determine an optimizer (pick between SGD or Adam).\n",
    "1. Instantiate a model and train the network with the created routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* [Training and Validation Loop - finetuning torch vision models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code)\n",
    "* [Simple Training Loop (no validation) - pytorch image classification](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MRConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MRConvNet(\n",
       "  (conv1): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (bnorm): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop1): Dropout3d(p=0.3, inplace=False)\n",
       "  (conv2): Conv3d(16, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / Test / Validation Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many files do we have?\n",
    "nda = NiftiDataset('./data/small/t1', './data/small/t2')\n",
    "corpus_size = len(nda)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# train, test, validation split\n",
    "corpus_idxs = range(0, corpus_size)\n",
    "train_idxs, test_idxs   = train_test_split(corpus_idxs, random_state=seed, train_size=0.75)\n",
    "train_idxs, valid_idxs  = train_test_split(train_idxs,  random_state=seed, train_size=0.8) #split training into training and validation.\n",
    "\n",
    "index_dict = {\n",
    "    'train': train_idxs, \n",
    "    'test': test_idxs,\n",
    "    'valid': valid_idxs\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DataLoaders and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "cube_crop_dim = 60\n",
    "mult = 7 #useful to acheive a similar effect to data augmentation. Multiplies each dataset by X.  Only works if you randomly crop. \n",
    "\n",
    "image_datasets   = {k:NiftiSplitDataset('./data/small/t1', './data/small/t2', \n",
    "                                        index_dict[k], \n",
    "                                        transforms.Compose([RandomCrop3D(cube_crop_dim), \n",
    "                                                            ToNumpy(), \n",
    "                                                            AddDim(), \n",
    "                                                            ToTensor()]),\n",
    "                                       mult)\n",
    "                    for k in ['train', 'valid', 'test']}\n",
    "dataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) \n",
    "                    for x in ['train', 'valid', 'test']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training / Validation loop\n",
    "\n",
    "Remind from Manning PyTorch book about the optimizer / loss and accumulation of gradients.\n",
    "\n",
    "> calling `backward()` will lead derivatives to accumulate at leaf nodes. So if backward has been called earlier, the loss is evaluated again and backward is called again (as in any training loop), the gradient at each leaf will be accumulated (i.e. summed) on top of the one computed at the previous iteration,\n",
    "\n",
    "In the case above the graph is a computation graph.  Operations can have other operations or tensors as inputs.  Tensors are the leafs in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            elif phase == 'valid':\n",
    "                model.eval()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "        \n",
    "            #loop over all batch of samples / pairs\n",
    "            for samples in dataloaders[phase]:\n",
    "            \n",
    "                # how does this work? \n",
    "                # default collate_fn magically handles it - https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
    "\n",
    "                #put the batch on the device\n",
    "                source = samples['source'].to(device)\n",
    "                target = samples['target'].to(device)\n",
    "            \n",
    "                # reset parameter gradients after each batch \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                # you only need the to track the computation graph if you are going to calculate \n",
    "                # the gradient via backprop\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    predicted = model(source)\n",
    "                    loss = criterion(predicted, target) #this is just a node in the computation graph. Returns tensor.\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # propagates gradients\n",
    "                        optimizer.step() # uses the learning rate and the gradients to update the weights. \n",
    "                        \n",
    "                # can we calculate any statistics for this batch? \n",
    "                ## - show off how many batches we are processing? \n",
    "                running_loss += loss.item() * samples['source'].size(0)\n",
    "                \n",
    "            # calculate any statistics for the phase + epoch?\n",
    "            ## - print training loss, print validation loss.\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "        \n",
    "    # calculate / report on anything for the epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.6566\n",
      "valid Loss: 0.5308\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.6008\n",
      "valid Loss: 0.5160\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.6120\n",
      "valid Loss: 0.5083\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.6247\n",
      "valid Loss: 0.4606\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.5950\n",
      "valid Loss: 0.4466\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6004\n",
      "valid Loss: 0.4909\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.5676\n",
      "valid Loss: 0.4731\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5804\n",
      "valid Loss: 0.4278\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.5734\n",
      "valid Loss: 0.4589\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.6135\n",
      "valid Loss: 0.4474\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.5853\n",
      "valid Loss: 0.4698\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.5732\n",
      "valid Loss: 0.4604\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.6029\n",
      "valid Loss: 0.4640\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6017\n",
      "valid Loss: 0.4495\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.5589\n",
      "valid Loss: 0.4526\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.5876\n",
      "valid Loss: 0.4547\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5864\n",
      "valid Loss: 0.4565\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5547\n",
      "valid Loss: 0.4223\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.5875\n",
      "valid Loss: 0.4806\n",
      "Epoch 19/49\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloaders_dict, criterion, optimizer, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Left off here\n",
    "\n",
    "'''\n",
    "\n",
    "I'd like get this model to not jump around so much. It should \n",
    "\n",
    "* Get more data\n",
    "    - get more random crops\n",
    "    - can you make smaller crops of the data? This would give you a chance to reuse images. \n",
    "\n",
    "* Make your model better\n",
    "    - check the Mannging book on Pytorch. What did their model look like? \n",
    "    \n",
    "* Experiment with the learning rate, batch_size and mult? \n",
    "    - can you get the training loss to decrease\n",
    "    - how would you graph the validation and training loss. \n",
    "    - do you want to run for a few epochs with a high learning rate then decrease it? \n",
    "    \n",
    "Questions\n",
    "\n",
    "* How would you create a full size image once the model is trained? It hasn't trained on any full size images. \n",
    "* What is a good validation loss? \n",
    "* is \"mult\" cheating too much? Can you come up with a better strategy? \n",
    "* do you want to save the \"best\" model that is not wildly overfitting (training loss not crazy less than validation loss)\n",
    "  and that has the best validation loss? \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Parameters\n",
      "conv1.weight torch.Size([16, 1, 3, 3, 3])\n",
      "conv1.bias torch.Size([16])\n",
      "bnorm.weight torch.Size([16])\n",
      "bnorm.bias torch.Size([16])\n",
      "conv2.weight torch.Size([1, 16, 3, 3, 3])\n",
      "conv2.bias torch.Size([1])\n",
      "\n",
      "All Parameters\n",
      "torch.Size([16, 1, 3, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 16, 3, 3, 3])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#what's in parameters? \n",
    "print(\"Named Parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"All Parameters\")\n",
    "        \n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
