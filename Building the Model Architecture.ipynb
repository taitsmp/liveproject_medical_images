{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Create a class that inherits from nn.Module and implement the following methods:\n",
    "   + `__init__` (put the convolutional blocks here)\n",
    "   + `forward` (this will take a properly formatted T1-w image as input and, once trained, output an approximate T2-w image of the same shape as the input image)\n",
    "1. Verify that the forward pass works by running an image from the training set through the network. Check that the input shape is the same as the output shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* Loss function will just compare T1 and T2 images\n",
    "* You many not need a loss function here. \n",
    "* Goal is simply to send an image through a forward pass of the network. Training happens in the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "* How would you reformulate this problem as a GAN?  Why would a GAN necessarily work better with less training data?\n",
    ">   Note that there are other techniques to accomplish this image transformation task (e.g., generative adversarial networks), but since we have a large set of paired data, we can use supervised techniques like we are doing here.\n",
    "* Downsample and upsample or just keep things simple and the same dimension? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Architecture \n",
    "class MRConvNet(nn.Module):\n",
    "    def __init__(self, nChans=[16]):\n",
    "        super(MRConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, nChans[0], 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate model and try out the forward() pass. \n",
    "mcn = MRConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(120, 120, 90)\n"
     ]
    }
   ],
   "source": [
    "# Load a raw image\n",
    "import nibabel as nib\n",
    "\n",
    "def load_mr_image(subj, ttype):\n",
    "    suff = ttype.upper()\n",
    "    if ttype == 't2': \n",
    "        suff = f'{suff}_reg'\n",
    "    return nib.load(f'./data/small/{ttype}/{subj}-{suff}_fcm.nii.gz')\n",
    "\n",
    "SUBJ = 'IXI102-HH-1416'\n",
    "\n",
    "image = load_mr_image(SUBJ, 't1')\n",
    "image_data = image.get_fdata()\n",
    "\n",
    "print(type(image_data))\n",
    "print(image_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 120, 120)\n"
     ]
    }
   ],
   "source": [
    "# Conver ND array to tensor and reshape it\n",
    "        \n",
    "# numpy image: H x W x C\n",
    "# torch image: C X H X W\n",
    "image_data = image_data.transpose((2, 0, 1))\n",
    "print(image_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 90, 120, 120])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.from_numpy(image_data)\n",
    "tensor = tensor.unsqueeze(0) # add channels\n",
    "tensor = tensor.unsqueeze(0) # add bactch (?)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider trying to load part of an image from the dataset here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = mcn.forward(tensor.float()) #this probably works as well. \n",
    "out = mcn(tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 88, 118, 118])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Left off here\n",
    "# - get back to 1 channel (not 16)\n",
    "# - zero pad the image (argument to Conv3d) so that you don't accidentally trim it with the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
